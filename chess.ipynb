{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import random\n",
    "\n",
    "!pip3 install python-chess\n",
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wrapper here makes sense because we need special stuff for the neural network, like get_encoded_state\n",
    "\n",
    "\n",
    "#TODO: implement change_perspective as flipping the board, but use the input plane to indicate the current player.\n",
    "# TODO: define game.action_size \n",
    "# TODO: define number of feature planes\n",
    "# TODO: implement all the stuff we saw in TicTacToe, will defer lots of it to python-chess, get_encoded_state will need to represent the board the way AlphaZero does.\n",
    "\n",
    "class ChessEncoder:\n",
    "\n",
    "    def generate_input_planes(self, board_history):\n",
    "        \"\"\"\n",
    "        Generates all 119 input planes for the AlphaZero chess neural network.\n",
    "        \n",
    "        :param board_history: A list of board states, with the most recent state last.\n",
    "        :param current_player: The current player (always presented as P1)\n",
    "        :return: A numpy array of shape (119, 8, 8), assuming an 8x8 chess board.\n",
    "        \n",
    "\n",
    "        Structure of 119 planes returned:\n",
    "            - 8x (representing up to 7 historical states, 1 current state), ordered from past to present:\n",
    "                - 6 P1 Piece Planes\n",
    "                - 6 P2 Piece Planes\n",
    "                - 2 Repetition Planes\n",
    "            - 1 Current player color plane\n",
    "            - 1 total move count plane\n",
    "            - 2 P1 Castling Planes, for Kingside and Queenside Castling\n",
    "            - 2 P2 Castling Planes, for Kingside and Queenside Castling\n",
    "            - 1 No-progress count plane\n",
    "\n",
    "        \"\"\"\n",
    "        all_planes = []\n",
    "\n",
    "        max_history_length = 8 # 7 previous board states + 1 current state\n",
    "\n",
    "        current_board = board_history[-1]\n",
    "        current_player_color = current_board.turn\n",
    "\n",
    "        for i in range(-max_history_length, 0):\n",
    "            if i < -len(board_history):\n",
    "                # For board states before the start of the game, add empty planes\n",
    "                all_planes.extend([np.zeros((8, 8)) for _ in range(14)])\n",
    "            else:\n",
    "                all_planes.extend(self.encode_state_at_time(board_history[i], current_player_color))\n",
    "        \n",
    "        # Add additional game state planes (assuming functions for each)\n",
    "        # These include player color, total move count, and special rules (castling, no-progress, etc.)\n",
    "        all_planes.append(self.encode_player_color_plane(current_player_color))\n",
    "        all_planes.append(self.encode_total_move_count_plane(current_board))\n",
    "        all_planes.extend(self.encode_castling_planes(current_board))\n",
    "        all_planes.append(self.encode_no_progress_count_plane(current_board))\n",
    "        \n",
    "        # Convert list of planes into a 3D numpy array (for the neural network)\n",
    "        return np.stack(all_planes)\n",
    "        \n",
    "\n",
    "    # Encodes the 14 planes for a specific time step.\n",
    "    def encode_state_at_time(self, board, current_player_color):\n",
    "        planes = []\n",
    "\n",
    "        opponent_player_color = not current_player_color\n",
    "\n",
    "        # Add 6 planes for P1's pieces (P1 is the current player)\n",
    "        for piece_type in chess.PIECE_TYPES:\n",
    "            planes.append(self.encode_piece_plane(board, piece_type, current_player_color))\n",
    "\n",
    "        # Add 6 planes for P2's pieces (P2 is the opponent)\n",
    "        for piece_type in chess.PIECE_TYPES:\n",
    "            planes.append(self.encode_piece_plane(board, piece_type, opponent_player_color))\n",
    "\n",
    "        # Add 2 repetition planes\n",
    "        planes.extend(self.encode_repetition_planes(board))\n",
    "\n",
    "        return planes\n",
    "\n",
    "    def encode_piece_plane(self, board, piece_type, color):\n",
    "        plane = np.zeros((8,8))\n",
    "\n",
    "        for board_idx in board.pieces(piece_type=piece_type, color=color):\n",
    "            row, col = divmod(board_idx, 8)\n",
    "            plane[row, col] = 1\n",
    "        \n",
    "        return plane\n",
    "\n",
    "\n",
    "    def encode_repetition_planes(self, board):\n",
    "        planes = []\n",
    "\n",
    "        if board.is_repetition(2): # one-fold repitition\n",
    "            planes.append(np.ones((8,8)))\n",
    "            if board.is_repetition(3): #two-fold repitition\n",
    "                planes.append(np.ones((8,8)))\n",
    "            else:\n",
    "                planes.append(np.zeros((8,8)))\n",
    "        else:\n",
    "            planes.append(np.zeros((8,8)))\n",
    "            planes.append(np.zeros((8,8)))\n",
    "\n",
    "        return planes\n",
    "\n",
    "    # I like the symmetry of using 1 and -1 (from player) here, rather than 1 and 0\n",
    "    def encode_player_color_plane(self, current_player_color):\n",
    "        if current_player_color == chess.WHITE:\n",
    "            player = 1\n",
    "        elif (current_player_color == chess.BLACK):\n",
    "            player = -1\n",
    "        else:\n",
    "            raise 'Missing board turn'\n",
    "        \n",
    "        return np.full((8,8), player)\n",
    "\n",
    "    def encode_total_move_count_plane(self, board):\n",
    "        return np.full((8,8), board.fullmove_number)\n",
    "\n",
    "    def encode_castling_planes(self, board):\n",
    "        planes = []\n",
    "\n",
    "        current_player_color = board.turn\n",
    "        opponent_player_color = not current_player_color\n",
    "\n",
    "        planes.append(np.full((8,8), int(board.has_kingside_castling_rights(current_player_color))))\n",
    "        planes.append(np.full((8,8), int(board.has_queenside_castling_rights(current_player_color))))\n",
    "\n",
    "        planes.append(np.full((8,8), int(board.has_kingside_castling_rights(opponent_player_color))))\n",
    "        planes.append(np.full((8,8), int(board.has_queenside_castling_rights(opponent_player_color))))\n",
    "\n",
    "        return planes\n",
    "\n",
    "    def encode_no_progress_count_plane(self, board):\n",
    "        return np.full((8,8), board.halfmove_clock)\n",
    "    \n",
    "\n",
    "# Input planes:\n",
    "    # 8x history of:\n",
    "        # 6 P1 Piece Planes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "True\n",
      "False\n",
      "range(0, 64)\n"
     ]
    }
   ],
   "source": [
    "print(list(chess.PIECE_TYPES))\n",
    "print(chess.PAWN)\n",
    "print(chess.KNIGHT)\n",
    "print(chess.BISHOP)\n",
    "print(chess.ROOK)\n",
    "print(chess.QUEEN)\n",
    "print(chess.KING)\n",
    "print(chess.WHITE)\n",
    "print(chess.BLACK)\n",
    "\n",
    "print(chess.SQUARES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up basic self play so I get encodings working\n",
    "import chess.svg\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import numpy as np\n",
    "\n",
    "encoder = ChessEncoder()\n",
    "\n",
    "# Initialize the chess board\n",
    "board = chess.Board()\n",
    "\n",
    "def display_board(board, use_svg=True):\n",
    "    if use_svg:\n",
    "        return display(HTML(chess.svg.board(board=board, size=400)))\n",
    "    else:\n",
    "        print(board)\n",
    "\n",
    "        \n",
    "# def play_move_interactive():\n",
    "#     display_board(board)\n",
    "#     move = input(\"Enter your move: \")\n",
    "#     try:\n",
    "#         board.push_san(move)\n",
    "#     except ValueError as e:\n",
    "#         print(f\"Invalid move: {e}\")\n",
    "#     clear_output(wait=True)\n",
    "#     display_board(board)\n",
    "\n",
    "# play_move_interactive()\n",
    "\n",
    "# board = board.transform(chess.flip_vertical)\n",
    "# board = board.transform(chess.flip_vertical)\n",
    "# display_board(board)\n",
    "\n",
    "# Confirmed, board.transform(chess.flip_vertical) works.\n",
    "\n",
    "\n",
    "board = chess.Board()\n",
    "\n",
    "encoded_state = encoder.generate_input_planes([board])\n",
    "\n",
    "print(encoded_state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use this cell to get it doing 1v1 and possibly pure MCTS play... seems maybe better than jumping right into the NN part of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: this needs updates.  board_size is not moves like in TicTacToe or Chess.  Need to actually think about the State we're passing in.. proly matching the alphazero.\n",
    "    # TODO: define feature_planes, which the initial convolution depends on.\n",
    "    # TODO: verify that we're passing in 8x8 sized feature planes, then we can just use game.row_count, game.column_count\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_filters, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        feature_planes = 3 # This will differ once we move beyond tic tac toe and connect four into chess\n",
    "        board_size = game.row_count * game.column_count\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(feature_planes, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for i in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Note, I changed the output channels on policy head from 32->2 compared to the code.  Also removed padding.\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 2, kernel_size=1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * board_size, game.action_size)\n",
    "        )\n",
    "\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(board_size, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_filters):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QSTN: Will we pass along states using FEN?\n",
    "# QSTN: do we want to change the game perspective like we've been doing?  So that the player always thinks they're p1 (white?).\n",
    "# TODO: we'll need to translate between the outputs of the neural network and specific chess actions\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "        \n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # We want minimal q_value for child, because it's our opponent.\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1) # The board is always from perspective of P1 moving\n",
    "                child_state = self.game.change_perspective(child_state)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "        return child\n",
    "    \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None: # True outside root node\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "    \n",
    "    @torch.no_grad() # Don't use MCTS for training neural network parameters\n",
    "    def search(self, states, spGames):\n",
    "        # Increased temp for start policy\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        # Dirichlet noise\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "\n",
    "        for _ in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                # Selection\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "                \n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value) # The value is from the perspective of the opponent of the person who made the move.\n",
    "\n",
    "                if is_terminal:\n",
    "                    # Back Propagation\n",
    "                    node.backpropagate(value)\n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "\n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "\n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves # only consider legal moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                # Expansion\n",
    "                node.expand(spg_policy)\n",
    "\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SelfPlayGame(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "\n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "\n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                # Introduce temperature into probs\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "            return return_memory\n",
    "\n",
    "    def train(self, memory):\n",
    "        # Want to shuffle training data to avoid getting same batches all the time\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) -1,batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Main method, runs self-play, and uses that data for training.\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in range(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "\n",
    "class SelfPlayGame:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval, for when the time comes...\n",
    "\n",
    "game = Chess()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
